# -*- coding: utf-8 -*-
"""T_winequality.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wJ5R7J-C9b-7hvVHxMz4eOiF3xb_WRsy
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load data
data = pd.read_csv('/content/winequality-red.csv')

# Features and target variable
X = data.drop('quality', axis=1)
y = data['quality']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Train the model
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)

# Predictions
y_pred = log_reg.predict(X_test)

# Evaluation
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.tree import DecisionTreeClassifier

# Train the model
decision_tree = DecisionTreeClassifier()
decision_tree.fit(X_train, y_train)

# Predictions
y_pred = decision_tree.predict(X_test)

# Evaluation
print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier

# Train the model
random_forest = RandomForestClassifier()
random_forest.fit(X_train, y_train)

# Predictions
y_pred = random_forest.predict(X_test)

# Evaluation
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)
X_poly = poly.fit_transform(X)

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

estimators = [
    ('rf', RandomForestClassifier(n_estimators=100)),
    ('gb', GradientBoostingClassifier(n_estimators=100))
]
stacking_model = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression()
)
stacking_model.fit(X_train_scaled, y_train)

from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000)
mlp.fit(X_train_scaled, y_train)

from sklearn.svm import SVC

svm_model = SVC(kernel='rbf', C=1.0)
svm_model.fit(X_train_scaled, y_train)

!pip install catboost
!pip install lightgbm
!pip install xgboost catboost lightgbm scikit-learn imbalanced-learn

from catboost import CatBoostClassifier

catboost_model = CatBoostClassifier(iterations=500, learning_rate=0.03, depth=6)
catboost_model.fit(X_train, y_train, verbose=0)

# Import required libraries
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, classification_report
from sklearn.neural_network import MLPClassifier
from catboost import CatBoostClassifier

# Load the dataset
df = pd.read_csv('/content/winequality-red.csv')

# Separate features and target variable
X = df.drop('quality', axis=1)
y = df['quality']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Optionally, apply Polynomial Features
poly = PolynomialFeatures(degree=2, interaction_only=True)
X_train_poly = poly.fit_transform(X_train_scaled)
X_test_poly = poly.transform(X_test_scaled)

# Apply PCA for dimensionality reduction (optional)
pca = PCA(n_components=10)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Initialize models (excluding XGBoost)
models = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'SVM': SVC(probability=True),
    'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000),
    'CatBoost': CatBoostClassifier(verbose=0)
}

# Train and evaluate models
for model_name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    print(f'{model_name} Accuracy: {accuracy_score(y_test, y_pred):.4f}')
    print(f'{model_name} Classification Report:')
    print(classification_report(y_test, y_pred))

# Voting Classifier (Ensemble of multiple models)
voting_clf = VotingClassifier(
    estimators=[
        ('rf', RandomForestClassifier()),
        ('gb', GradientBoostingClassifier()),
        ('svm', SVC(probability=True))
    ],
    voting='soft'
)
voting_clf.fit(X_train_scaled, y_train)
y_pred_voting = voting_clf.predict(X_test_scaled)
print(f'Voting Classifier Accuracy: {accuracy_score(y_test, y_pred_voting):.4f}')
print('Voting Classifier Classification Report:')
print(classification_report(y_test, y_pred_voting))

# Hyperparameter Tuning using GridSearchCV for RandomForest
param_grid = {
    'n_estimators': [100, 300, 500],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10]
}
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_scaled, y_train)
best_rf = grid_search.best_estimator_
y_pred_rf = best_rf.predict(X_test_scaled)
print(f'Best Random Forest Accuracy after GridSearch: {accuracy_score(y_test, y_pred_rf):.4f}')
print('Best Random Forest Classification Report:')
print(classification_report(y_test, y_pred_rf))

# Evaluate MLP (Neural Network)
mlp_model = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000)
mlp_model.fit(X_train_scaled, y_train)
y_pred_mlp = mlp_model.predict(X_test_scaled)
print(f'MLP Neural Network Accuracy: {accuracy_score(y_test, y_pred_mlp):.4f}')
print('MLP Neural Network Classification Report:')
print(classification_report(y_test, y_pred_mlp))

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report
from sklearn.svm import SVC
from catboost import CatBoostClassifier
from imblearn.over_sampling import SMOTE

# Load dataset and split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Address class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)


# Random Forest with hyperparameter tuning
rf_params = {'n_estimators': [100, 200], 'max_depth': [10, 15], 'min_samples_split': [2, 5]}
rf = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=3)
rf.fit(X_train_resampled, y_train_resampled)
rf_pred = rf.predict(X_test_scaled)
print("Random Forest Classification Report:\n", classification_report(y_test, rf_pred))

# Gradient Boosting with hyperparameter tuning
gb_params = {'n_estimators': [100, 200], 'learning_rate': [0.1, 0.01], 'max_depth': [3, 5]}
gb = GridSearchCV(GradientBoostingClassifier(random_state=42), gb_params, cv=3)
gb.fit(X_train_resampled, y_train_resampled)
gb_pred = gb.predict(X_test_scaled)
print("Gradient Boosting Classification Report:\n", classification_report(y_test, gb_pred))

# Neural Network with optimized architecture
mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
mlp.fit(X_train_resampled, y_train_resampled)
mlp_pred = mlp.predict(X_test_scaled)
print("MLP Neural Network Classification Report:\n", classification_report(y_test, mlp_pred))

# CatBoost Classifier with hyperparameter tuning
cat = CatBoostClassifier(iterations=500, depth=6, learning_rate=0.1, verbose=0, random_state=42)
cat.fit(X_train_resampled, y_train_resampled)
cat_pred = cat.predict(X_test_scaled)
print("CatBoost Classification Report:\n", classification_report(y_test, cat_pred))

# Voting Classifier (Ensemble of the best models)
voting_clf = VotingClassifier(estimators=[
    ('lr', log_reg), ('rf', rf.best_estimator_), ('gb', gb.best_estimator_), ('mlp', mlp), ('cat', cat)], voting='soft')
voting_clf.fit(X_train_resampled, y_train_resampled)
voting_pred = voting_clf.predict(X_test_scaled)
print("Voting Classifier Classification Report:\n", classification_report(y_test, voting_pred))